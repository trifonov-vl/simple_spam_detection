{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from urlextract import URLExtract\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, raw_text, label=None, tokens=None):\n",
    "        self._raw_text = raw_text\n",
    "        self._label = label\n",
    "        self._tokens = None\n",
    "        if tokens is not None:\n",
    "            self._tokens = list(tokens)\n",
    "    \n",
    "    @property\n",
    "    def raw_text(self):\n",
    "        return self._raw_text\n",
    "    \n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._label\n",
    "    \n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return list(self._tokens)\n",
    "    \n",
    "    def tokenized(self, tokenizer):\n",
    "        return Document(self._raw_text, self._label, tokenizer.tokenize(self._raw_text))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self._label}\\t{self._raw_text}\\t{self._tokens if self._tokens is not None else ''}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_labels(docs):\n",
    "    return (d.label for d in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path, delimiter=',', encoding='latin-1')\n",
    "    return [Document(text, label) for text, label in zip(df.v2, df.v1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_dataset(\"spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\t,\n",
       " ham\tOk lar... Joking wif u oni...\t,\n",
       " spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\t,\n",
       " ham\tU dun say so early hor... U c already then say...\t,\n",
       " ham\tNah I don't think he goes to usf, he lives around here though\t]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 4825, 'spam': 747})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(docs_labels(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\n",
      "\n",
      "WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "\n",
      "URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "\n",
      "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
      "\n",
      "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ì¼1.20 POBOXox36504W45WQ 16+\n",
      "\n",
      "Thanks for your subscription to Ringtone UK your mobile will be charged å£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\n",
      "\n",
      "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow\n",
      "\n",
      "SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV\n",
      "\n",
      "Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out! \n",
      "\n",
      "As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a å£1500 Bonus Prize, call 09066364589\n",
      "\n",
      "Urgent UR awarded a complimentary trip to EuroDisinc Trav, Aco&Entry41 Or å£1000. To claim txt DIS to 87121 18+6*å£1.50(moreFrmMob. ShrAcomOrSglSuplt)10, LS1 3AJ\n",
      "\n",
      "Did you hear about the new \\Divorce Barbie\\\"? It comes with all of Ken's stuff!\"\n",
      "\n",
      "Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WON a guaranteed å£1000 cash or å£5000 prize!\n",
      "\n",
      "Your free ringtone is waiting to be collected. Simply text the password \\MIX\\\" to 85069 to verify. Get Usher and Britney. FML\n",
      "\n",
      "GENT! We are trying to contact you. Last weekends draw shows that you won a å£1000 prize GUARANTEED. Call 09064012160. Claim Code K52. Valid 12hrs only. 150ppm\n",
      "\n",
      "You are a winner U have been specially selected 2 receive å£1000 or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810910p/min (18+) \n",
      "\n",
      "PRIVATE! Your 2004 Account Statement for 07742676969 shows 786 unredeemed Bonus Points. To claim call 08719180248 Identifier Code: 45239 Expires\n",
      "\n",
      "URGENT! Your Mobile No. was awarded å£2000 Bonus Caller Prize on 5/9/03 This is our final try to contact U! Call from Landline 09064019788 BOX42WR29C, 150PPM\n",
      "\n",
      "Todays Voda numbers ending 7548 are selected to receive a $350 award. If you have a match please call 08712300220 quoting claim code 4041 standard rates app\n",
      "\n",
      "Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country the Algarve is in? Txt ansr to 82277. å£1.50 SP:Tyrone\n",
      "\n",
      "Want 2 get laid tonight? Want real Dogging locations sent direct 2 ur mob? Join the UK's largest Dogging Network bt Txting GRAVEL to 69888! Nt. ec2a. 31p.msg@150p\n",
      "\n",
      "You'll not rcv any more msgs from the chat svc. For FREE Hardcore services text GO to: 69988 If u get nothing u must Age Verify with yr network & try again\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(d for d in docs if d.label == \"spam\"):\n",
    "    if i >= 25:\n",
    "        break\n",
    "    print(doc.raw_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNLTKTokenizer:\n",
    "    DIGITS_RE = re.compile(r'[0-9]+')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "        self.url_extractor = URLExtract()\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        text = self.DIGITS_RE.sub('0', text)\n",
    "        tokens = []\n",
    "        for sent in self.sent_detector.tokenize(text):\n",
    "            urls = self.url_extractor.find_urls(sent, only_unique=True)\n",
    "            for url in urls:\n",
    "                sent = sent.replace(url, \"<href>\")\n",
    "            tokens.extend(self.tokenizer.tokenize(sent))            \n",
    "                \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TwitterNLTKTokenizer()\n",
    "docs = [d.tokenized(tokenizer) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\t['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...']\n",
      "\n",
      "ham\tOk lar... Joking wif u oni...\t['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...']\n",
      "\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\t['free', 'entry', 'in', '0', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '0st', 'may', '0', '.', 'text', 'fa', 'to', '0', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 't', '&', \"c's\", 'apply', '0over0', \"'\", 's']\n",
      "\n",
      "ham\tU dun say so early hor... U c already then say...\t['u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...']\n",
      "\n",
      "ham\tNah I don't think he goes to usf, he lives around here though\t['nah', 'i', \"don't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in docs[:5]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TfIDFFeatureExtractor:\n",
    "    def __init__(self, docs):\n",
    "        self._vectorizer = TfidfVectorizer(\n",
    "            token_pattern=None,\n",
    "            tokenizer=self._tokenize_doc,\n",
    "            stop_words='english', min_df=0.001,\n",
    "            preprocessor=self._identity\n",
    "        )\n",
    "        self._vectorizer.fit(docs)\n",
    "    \n",
    "    @property\n",
    "    def vector_length(self):\n",
    "        return len(self._vectorizer.vocabulary_)\n",
    "    \n",
    "    def vectorize_docs(self, docs):\n",
    "        return self._vectorizer.transform(docs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _tokenize_doc(doc):\n",
    "        return doc.tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def _identity(x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcherize(samples, batch_size):\n",
    "    queue = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        if len(queue) == batch_size:\n",
    "            yield queue\n",
    "            queue = []\n",
    "        \n",
    "        queue.append(sample)\n",
    "    \n",
    "    if queue:\n",
    "        yield queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(predicted_labels, gold_labels):\n",
    "    def to_int(labels, positive_label=\"spam\"):\n",
    "        return [int(l == positive_label) for l in labels]\n",
    "    \n",
    "    return f1_score(to_int(gold_labels), to_int(predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BinaryClassificationMLP(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_sizes: list, activation=torch.nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._activation = activation()\n",
    "        self._hidden_layers = []\n",
    "                \n",
    "        prev_size = input_size\n",
    "        for size in hidden_sizes:\n",
    "            layer = torch.nn.Linear(prev_size, size)\n",
    "            prev_size = size\n",
    "        \n",
    "        self._out_layer = torch.nn.Linear(prev_size, 1)\n",
    "        self._out_sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        for layer in self._hidden_layers:\n",
    "            sample = self._activation(layer(sample))\n",
    "        \n",
    "        return self._out_sigmoid(self._out_layer(sample))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def train_dev_test_split(docs, test_ratio, dev_ratio, random_seed=100):\n",
    "    def index_docs(docs, indexes):\n",
    "        return [docs[idx] for idx in indexes]\n",
    "        \n",
    "    train, dev_test = iter(next(\n",
    "        StratifiedShuffleSplit(1, test_ratio + dev_ratio, random_state=random_seed).split(\n",
    "            docs, list(docs_labels(docs)))))\n",
    "    \n",
    "    train_docs = index_docs(docs, train)\n",
    "    dev_test_docs = index_docs(docs, dev_test)\n",
    "    \n",
    "    dev, test = iter(next(\n",
    "        StratifiedShuffleSplit(1, test_ratio / (dev_ratio + test_ratio), random_state=random_seed).split(\n",
    "            dev_test_docs, list(docs_labels(dev_test_docs)))))\n",
    "    \n",
    "    return train_docs, index_docs(dev_test_docs, dev), index_docs(dev_test_docs, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = train_dev_test_split(docs, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
