{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from urlextract import URLExtract\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple datamodel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, raw_text, label=None, tokens=None):\n",
    "        self._raw_text = raw_text\n",
    "        self._label = label\n",
    "        self._tokens = None\n",
    "        if tokens is not None:\n",
    "            self._tokens = list(tokens)\n",
    "    \n",
    "    @property\n",
    "    def raw_text(self):\n",
    "        return self._raw_text\n",
    "    \n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._label\n",
    "    \n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return list(self._tokens)\n",
    "    \n",
    "    def tokenized(self, tokenizer):\n",
    "        return Document(self._raw_text, self._label, tokenizer.tokenize(self._raw_text))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self._label}\\t{self._raw_text}\\t{self._tokens if self._tokens is not None else ''}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful function to get iterator over docs labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_labels(docs):\n",
    "    return (d.label for d in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw data in our datamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path, delimiter=',', encoding='latin-1')\n",
    "    return [Document(text, label) for text, label in zip(df.v2, df.v1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_dataset(\"spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\t,\n",
       " ham\tOk lar... Joking wif u oni...\t,\n",
       " spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\t,\n",
       " ham\tU dun say so early hor... U c already then say...\t,\n",
       " ham\tNah I don't think he goes to usf, he lives around here though\t]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 4825, 'spam': 747})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(docs_labels(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check SMS that we are interested to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\n",
      "\n",
      "WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "\n",
      "URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "\n",
      "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
      "\n",
      "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ì¼1.20 POBOXox36504W45WQ 16+\n",
      "\n",
      "Thanks for your subscription to Ringtone UK your mobile will be charged å£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\n",
      "\n",
      "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow\n",
      "\n",
      "SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV\n",
      "\n",
      "Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out! \n",
      "\n",
      "As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a å£1500 Bonus Prize, call 09066364589\n",
      "\n",
      "Urgent UR awarded a complimentary trip to EuroDisinc Trav, Aco&Entry41 Or å£1000. To claim txt DIS to 87121 18+6*å£1.50(moreFrmMob. ShrAcomOrSglSuplt)10, LS1 3AJ\n",
      "\n",
      "Did you hear about the new \\Divorce Barbie\\\"? It comes with all of Ken's stuff!\"\n",
      "\n",
      "Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WON a guaranteed å£1000 cash or å£5000 prize!\n",
      "\n",
      "Your free ringtone is waiting to be collected. Simply text the password \\MIX\\\" to 85069 to verify. Get Usher and Britney. FML\n",
      "\n",
      "GENT! We are trying to contact you. Last weekends draw shows that you won a å£1000 prize GUARANTEED. Call 09064012160. Claim Code K52. Valid 12hrs only. 150ppm\n",
      "\n",
      "You are a winner U have been specially selected 2 receive å£1000 or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810910p/min (18+) \n",
      "\n",
      "PRIVATE! Your 2004 Account Statement for 07742676969 shows 786 unredeemed Bonus Points. To claim call 08719180248 Identifier Code: 45239 Expires\n",
      "\n",
      "URGENT! Your Mobile No. was awarded å£2000 Bonus Caller Prize on 5/9/03 This is our final try to contact U! Call from Landline 09064019788 BOX42WR29C, 150PPM\n",
      "\n",
      "Todays Voda numbers ending 7548 are selected to receive a $350 award. If you have a match please call 08712300220 quoting claim code 4041 standard rates app\n",
      "\n",
      "Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country the Algarve is in? Txt ansr to 82277. å£1.50 SP:Tyrone\n",
      "\n",
      "Want 2 get laid tonight? Want real Dogging locations sent direct 2 ur mob? Join the UK's largest Dogging Network bt Txting GRAVEL to 69888! Nt. ec2a. 31p.msg@150p\n",
      "\n",
      "You'll not rcv any more msgs from the chat svc. For FREE Hardcore services text GO to: 69988 If u get nothing u must Age Verify with yr network & try again\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(d for d in docs if d.label == \"spam\"):\n",
    "    if i >= 25:\n",
    "        break\n",
    "    print(doc.raw_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So many digits and urls. Let's use twitter-based NLTK tokenizer and do some preprocessing: change all digits to one 0 and change all urls to '<href>' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNLTKTokenizer:\n",
    "    DIGITS_RE = re.compile(r'[0-9]+')\n",
    "    \n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        self.sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "        self.url_extractor = URLExtract()\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        text = self.DIGITS_RE.sub('0', text)\n",
    "        tokens = []\n",
    "        for sent in self.sent_detector.tokenize(text):\n",
    "            urls = self.url_extractor.find_urls(sent, only_unique=True)\n",
    "            for url in urls:\n",
    "                sent = sent.replace(url, \"<href>\")\n",
    "            tokens.extend(self.tokenizer.tokenize(sent))            \n",
    "                \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vladislav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TwitterNLTKTokenizer()\n",
    "docs = [d.tokenized(tokenizer) for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\t['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...']\n",
      "\n",
      "ham\tOk lar... Joking wif u oni...\t['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...']\n",
      "\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\t['free', 'entry', 'in', '0', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '0st', 'may', '0', '.', 'text', 'fa', 'to', '0', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 't', '&', \"c's\", 'apply', '0over0', \"'\", 's']\n",
      "\n",
      "ham\tU dun say so early hor... U c already then say...\t['u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...']\n",
      "\n",
      "ham\tNah I don't think he goes to usf, he lives around here though\t['nah', 'i', \"don't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in docs[:5]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do simple TFIDF features for our small texts, may be we can find some keywords through them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIDFFeatureExtractor:\n",
    "    def __init__(self, docs):\n",
    "        self._vectorizer = TfidfVectorizer(\n",
    "            token_pattern=None,\n",
    "            tokenizer=self._tokenize_doc,\n",
    "            stop_words='english', min_df=0.001,\n",
    "            preprocessor=self._identity\n",
    "        )\n",
    "        self._vectorizer.fit(docs)\n",
    "    \n",
    "    @property\n",
    "    def vector_length(self):\n",
    "        return len(self._vectorizer.vocabulary_)\n",
    "    \n",
    "    def vectorize_docs(self, docs):\n",
    "        return [f.toarray()[0] for f in self._vectorizer.transform(docs)]\n",
    "        \n",
    "    @staticmethod\n",
    "    def _tokenize_doc(doc):\n",
    "        return doc.tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def _identity(x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to iterate over dataset, generate features and collect samples into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "def batcherize(docs, feature_extractor, batch_size, shuffle=True, labels=True):\n",
    "    if shuffle:\n",
    "        docs = list(docs)\n",
    "        random.shuffle(docs)\n",
    "    \n",
    "    samples = feature_extractor.vectorize_docs(docs)\n",
    "    labels = docs_labels(docs) if labels else repeat(None)\n",
    "    \n",
    "    samples_queue, labels_queue = [], []\n",
    "    \n",
    "    for sample, label in zip(samples, labels):\n",
    "        if len(samples_queue) == batch_size:\n",
    "            yield samples_queue, labels_queue\n",
    "            samples_queue, labels_queue = [], []\n",
    "        \n",
    "        samples_queue.append(sample)\n",
    "        labels_queue.append(1 if label == \"spam\" else 0)\n",
    "    \n",
    "    if samples_queue:\n",
    "        yield samples_queue, labels_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate with F1 because we have imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predicted_labels, gold_labels):\n",
    "    def to_int(labels, positive_label=\"spam\"):\n",
    "        return [int(l == positive_label) for l in labels]\n",
    "    return f1_score(to_int(gold_labels), to_int(predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple pytorch multilayered perceptron network for binary classification with sigmoid output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationMLP(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_sizes: list, activation=torch.nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._activation = activation()\n",
    "        self._hidden_layers = []\n",
    "                \n",
    "        prev_size = input_size\n",
    "        for size in hidden_sizes:\n",
    "            layer = torch.nn.Linear(prev_size, size)\n",
    "            self._hidden_layers.append(layer)\n",
    "            prev_size = size\n",
    "        \n",
    "        self._out_layer = torch.nn.Linear(prev_size, 1)\n",
    "        self._out_sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        for layer in self._hidden_layers:\n",
    "            sample = self._activation(layer(sample))\n",
    "        \n",
    "        return self._out_sigmoid(self._out_layer(sample))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need docs to train and to evaluate, let's split them. Also we are accurate with class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_split(docs, test_ratio, dev_ratio, random_seed=100):\n",
    "    def index_docs(docs, indexes):\n",
    "        return [docs[idx] for idx in indexes]\n",
    "        \n",
    "    train, dev_test = iter(next(\n",
    "        StratifiedShuffleSplit(1, test_ratio + dev_ratio, random_state=random_seed).split(\n",
    "            docs, list(docs_labels(docs)))))\n",
    "    \n",
    "    train_docs = index_docs(docs, train)\n",
    "    dev_test_docs = index_docs(docs, dev_test)\n",
    "    \n",
    "    dev, test = iter(next(\n",
    "        StratifiedShuffleSplit(1, test_ratio / (dev_ratio + test_ratio), random_state=random_seed).split(\n",
    "            dev_test_docs, list(docs_labels(dev_test_docs)))))\n",
    "    \n",
    "    return train_docs, index_docs(dev_test_docs, dev), index_docs(dev_test_docs, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = train_dev_test_split(docs, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get predictions from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dev, batcher_factory):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for samples, _ in batcher_factory(dev, False, False):\n",
    "            outputs = model(torch.tensor(samples, dtype=torch.float32))\n",
    "            predictions.extend(l[0] for l in outputs.numpy())\n",
    "    return [\"spam\" if p > 0.5 else \"ham\" for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train model, returns max score we got on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch_num, train, dev, batcher_factory, model, optimizer, criterion):\n",
    "    max_dev_score = 0\n",
    "    for epoch in range(epoch_num):\n",
    "        running_loss = 0.0\n",
    "        for i, (samples, labels) in enumerate(batcher_factory(train, True, True)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(torch.tensor(samples, dtype=torch.float32))\n",
    "            loss = criterion(outputs, torch.tensor(labels, dtype=torch.float32).unsqueeze_(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 300 == 299:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "        print(f\"train score = {evaluate(predict(model, train, batcher_factory), docs_labels(train))}\")\n",
    "        dev_score = evaluate(predict(model, dev, batcher_factory), docs_labels(dev))\n",
    "        print(f\"dev score = {dev_score}\")\n",
    "        max_dev_score = max(dev_score, max_dev_score)\n",
    "    \n",
    "    return max_dev_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factory for our batcher with feature extractor. Need to decompose batcher and feature extractor but not enough motivation to do it in pet project :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batcher_factory(feature_extractor, batch_size):\n",
    "    return lambda docs, shuffle, labels: batcherize(docs, feature_extractor, batch_size, shuffle, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment pipeline. Evaluate model with 5 seeds and make results reproducible by setting seeds everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.048\n",
      "[1,   600] loss: 0.038\n",
      "train score = 0.6169665809768637\n",
      "dev score = 0.6628571428571429\n",
      "[2,   300] loss: 0.028\n",
      "[2,   600] loss: 0.025\n",
      "train score = 0.7671541057367829\n",
      "dev score = 0.792079207920792\n",
      "[3,   300] loss: 0.024\n",
      "[3,   600] loss: 0.021\n",
      "train score = 0.7951807228915663\n",
      "dev score = 0.8155339805825244\n",
      "[4,   300] loss: 0.021\n",
      "[4,   600] loss: 0.020\n",
      "train score = 0.801317233809001\n",
      "dev score = 0.8195121951219512\n",
      "[5,   300] loss: 0.019\n",
      "[5,   600] loss: 0.019\n",
      "train score = 0.8180839612486545\n",
      "dev score = 0.8285714285714286\n",
      "[6,   300] loss: 0.018\n",
      "[6,   600] loss: 0.018\n",
      "train score = 0.7968923418423973\n",
      "dev score = 0.8078817733990148\n",
      "[7,   300] loss: 0.015\n",
      "[7,   600] loss: 0.017\n",
      "train score = 0.8392484342379959\n",
      "dev score = 0.839622641509434\n",
      "[8,   300] loss: 0.016\n",
      "[8,   600] loss: 0.016\n",
      "train score = 0.8841285296981499\n",
      "dev score = 0.8558558558558559\n",
      "[9,   300] loss: 0.016\n",
      "[9,   600] loss: 0.015\n",
      "train score = 0.8467153284671532\n",
      "dev score = 0.8544600938967137\n",
      "[10,   300] loss: 0.015\n",
      "[10,   600] loss: 0.015\n",
      "train score = 0.8463157894736841\n",
      "dev score = 0.8490566037735849\n",
      "[11,   300] loss: 0.015\n",
      "[11,   600] loss: 0.015\n",
      "train score = 0.8416578108395324\n",
      "dev score = 0.8365384615384616\n",
      "[12,   300] loss: 0.015\n",
      "[12,   600] loss: 0.013\n",
      "train score = 0.8893320039880359\n",
      "dev score = 0.8687782805429863\n",
      "[13,   300] loss: 0.012\n",
      "[13,   600] loss: 0.016\n",
      "train score = 0.8701030927835052\n",
      "dev score = 0.8598130841121495\n",
      "[14,   300] loss: 0.015\n",
      "[14,   600] loss: 0.012\n",
      "train score = 0.896551724137931\n",
      "dev score = 0.8687782805429863\n",
      "[15,   300] loss: 0.013\n",
      "[15,   600] loss: 0.013\n",
      "train score = 0.898406374501992\n",
      "dev score = 0.8687782805429863\n",
      "[16,   300] loss: 0.014\n",
      "[16,   600] loss: 0.011\n",
      "train score = 0.8929292929292929\n",
      "dev score = 0.8715596330275229\n",
      "[17,   300] loss: 0.013\n",
      "[17,   600] loss: 0.014\n",
      "train score = 0.9023904382470119\n",
      "dev score = 0.8727272727272727\n",
      "[18,   300] loss: 0.012\n",
      "[18,   600] loss: 0.014\n",
      "train score = 0.885480572597137\n",
      "dev score = 0.8796296296296297\n",
      "[19,   300] loss: 0.013\n",
      "[19,   600] loss: 0.013\n",
      "train score = 0.9007782101167315\n",
      "dev score = 0.8648648648648648\n",
      "[20,   300] loss: 0.013\n",
      "[20,   600] loss: 0.013\n",
      "train score = 0.8663883089770354\n",
      "dev score = 0.8516746411483254\n",
      "[1,   300] loss: 0.051\n",
      "[1,   600] loss: 0.035\n",
      "train score = 0.6788766788766789\n",
      "dev score = 0.7027027027027029\n",
      "[2,   300] loss: 0.029\n",
      "[2,   600] loss: 0.027\n",
      "train score = 0.7667804323094427\n",
      "dev score = 0.7434554973821988\n",
      "[3,   300] loss: 0.024\n",
      "[3,   600] loss: 0.021\n",
      "train score = 0.8117519042437432\n",
      "dev score = 0.7960199004975124\n",
      "[4,   300] loss: 0.021\n",
      "[4,   600] loss: 0.019\n",
      "train score = 0.8521199586349534\n",
      "dev score = 0.8186046511627907\n",
      "[5,   300] loss: 0.018\n",
      "[5,   600] loss: 0.018\n",
      "train score = 0.8580310880829015\n",
      "dev score = 0.8169014084507042\n",
      "[6,   300] loss: 0.017\n",
      "[6,   600] loss: 0.017\n",
      "train score = 0.8678861788617886\n",
      "dev score = 0.8333333333333334\n",
      "[7,   300] loss: 0.015\n",
      "[7,   600] loss: 0.018\n",
      "train score = 0.8669438669438669\n",
      "dev score = 0.8151658767772512\n",
      "[8,   300] loss: 0.014\n",
      "[8,   600] loss: 0.017\n",
      "train score = 0.86875\n",
      "dev score = 0.8151658767772512\n",
      "[9,   300] loss: 0.015\n",
      "[9,   600] loss: 0.014\n",
      "train score = 0.8839103869653768\n",
      "dev score = 0.8425925925925926\n",
      "[10,   300] loss: 0.013\n",
      "[10,   600] loss: 0.015\n",
      "train score = 0.8928571428571428\n",
      "dev score = 0.8378378378378378\n",
      "[11,   300] loss: 0.015\n",
      "[11,   600] loss: 0.014\n",
      "train score = 0.8835758835758835\n",
      "dev score = 0.830188679245283\n",
      "[12,   300] loss: 0.014\n",
      "[12,   600] loss: 0.014\n",
      "train score = 0.8924949290060853\n",
      "dev score = 0.8440366972477065\n",
      "[13,   300] loss: 0.014\n",
      "[13,   600] loss: 0.014\n",
      "train score = 0.8941908713692945\n",
      "dev score = 0.839622641509434\n",
      "[14,   300] loss: 0.013\n",
      "[14,   600] loss: 0.015\n",
      "train score = 0.8983739837398375\n",
      "dev score = 0.8401826484018264\n",
      "[15,   300] loss: 0.014\n",
      "[15,   600] loss: 0.013\n",
      "train score = 0.9008264462809917\n",
      "dev score = 0.8450704225352113\n",
      "[16,   300] loss: 0.012\n",
      "[16,   600] loss: 0.014\n",
      "train score = 0.9061553985872857\n",
      "dev score = 0.8363636363636364\n",
      "[17,   300] loss: 0.014\n",
      "[17,   600] loss: 0.011\n",
      "train score = 0.9089048106448312\n",
      "dev score = 0.8518518518518519\n",
      "[18,   300] loss: 0.011\n",
      "[18,   600] loss: 0.013\n",
      "train score = 0.9030362389813907\n",
      "dev score = 0.8296943231441047\n",
      "[19,   300] loss: 0.013\n",
      "[19,   600] loss: 0.011\n",
      "train score = 0.9026036644165863\n",
      "dev score = 0.8296943231441047\n",
      "[20,   300] loss: 0.012\n",
      "[20,   600] loss: 0.013\n",
      "train score = 0.90748031496063\n",
      "dev score = 0.8333333333333334\n",
      "[1,   300] loss: 0.051\n",
      "[1,   600] loss: 0.035\n",
      "train score = 0.6448362720403022\n",
      "dev score = 0.6741573033707865\n",
      "[2,   300] loss: 0.028\n",
      "[2,   600] loss: 0.026\n",
      "train score = 0.780269058295964\n",
      "dev score = 0.7614213197969542\n",
      "[3,   300] loss: 0.025\n",
      "[3,   600] loss: 0.021\n",
      "train score = 0.7819209039548023\n",
      "dev score = 0.7653061224489796\n",
      "[4,   300] loss: 0.019\n",
      "[4,   600] loss: 0.021\n",
      "train score = 0.8057395143487859\n",
      "dev score = 0.7878787878787877\n",
      "[5,   300] loss: 0.018\n",
      "[5,   600] loss: 0.019\n",
      "train score = 0.8457502623294858\n",
      "dev score = 0.838095238095238\n",
      "[6,   300] loss: 0.019\n",
      "[6,   600] loss: 0.017\n",
      "train score = 0.824295010845987\n",
      "dev score = 0.8235294117647057\n",
      "[7,   300] loss: 0.016\n",
      "[7,   600] loss: 0.016\n",
      "train score = 0.8743718592964824\n",
      "dev score = 0.8623853211009174\n",
      "[8,   300] loss: 0.017\n",
      "[8,   600] loss: 0.016\n",
      "train score = 0.8175824175824176\n",
      "dev score = 0.8217821782178217\n",
      "[9,   300] loss: 0.018\n",
      "[9,   600] loss: 0.014\n",
      "train score = 0.8356605800214824\n",
      "dev score = 0.8390243902439024\n",
      "[10,   300] loss: 0.015\n",
      "[10,   600] loss: 0.015\n",
      "train score = 0.8902195608782435\n",
      "dev score = 0.8785046728971961\n",
      "[11,   300] loss: 0.015\n",
      "[11,   600] loss: 0.014\n",
      "train score = 0.8942115768463074\n",
      "dev score = 0.8785046728971961\n",
      "[12,   300] loss: 0.013\n",
      "[12,   600] loss: 0.016\n",
      "train score = 0.8734439834024895\n",
      "dev score = 0.8666666666666666\n",
      "[13,   300] loss: 0.015\n",
      "[13,   600] loss: 0.014\n",
      "train score = 0.899009900990099\n",
      "dev score = 0.8785046728971961\n",
      "[14,   300] loss: 0.013\n",
      "[14,   600] loss: 0.014\n",
      "train score = 0.8968779564806054\n",
      "dev score = 0.8789237668161436\n",
      "[15,   300] loss: 0.012\n",
      "[15,   600] loss: 0.014\n",
      "train score = 0.9018830525272548\n",
      "dev score = 0.8826291079812207\n",
      "[16,   300] loss: 0.014\n",
      "[16,   600] loss: 0.013\n",
      "train score = 0.9053254437869822\n",
      "dev score = 0.8785046728971961\n",
      "[17,   300] loss: 0.013\n",
      "[17,   600] loss: 0.013\n",
      "train score = 0.8857142857142857\n",
      "dev score = 0.8773584905660378\n",
      "[18,   300] loss: 0.010\n",
      "[18,   600] loss: 0.015\n",
      "train score = 0.8982035928143713\n",
      "dev score = 0.8826291079812207\n",
      "[19,   300] loss: 0.012\n",
      "[19,   600] loss: 0.014\n",
      "train score = 0.9073359073359073\n",
      "dev score = 0.8767123287671232\n",
      "[20,   300] loss: 0.013\n",
      "[20,   600] loss: 0.013\n",
      "train score = 0.9071358748778104\n",
      "dev score = 0.8837209302325582\n",
      "[1,   300] loss: 0.051\n",
      "[1,   600] loss: 0.037\n",
      "train score = 0.5105189340813465\n",
      "dev score = 0.5988023952095808\n",
      "[2,   300] loss: 0.030\n",
      "[2,   600] loss: 0.026\n",
      "train score = 0.6971153846153846\n",
      "dev score = 0.7263157894736842\n",
      "[3,   300] loss: 0.024\n",
      "[3,   600] loss: 0.021\n",
      "train score = 0.8200836820083682\n",
      "dev score = 0.8095238095238095\n",
      "[4,   300] loss: 0.023\n",
      "[4,   600] loss: 0.019\n",
      "train score = 0.7911111111111111\n",
      "dev score = 0.7777777777777778\n",
      "[5,   300] loss: 0.019\n",
      "[5,   600] loss: 0.021\n",
      "train score = 0.8276595744680851\n",
      "dev score = 0.821256038647343\n",
      "[6,   300] loss: 0.017\n",
      "[6,   600] loss: 0.020\n",
      "train score = 0.8580183861082737\n",
      "dev score = 0.8411214953271029\n",
      "[7,   300] loss: 0.018\n",
      "[7,   600] loss: 0.017\n",
      "train score = 0.8628628628628628\n",
      "dev score = 0.8387096774193549\n",
      "[8,   300] loss: 0.014\n",
      "[8,   600] loss: 0.020\n",
      "train score = 0.8697394789579158\n",
      "dev score = 0.8571428571428572\n",
      "[9,   300] loss: 0.017\n",
      "[9,   600] loss: 0.016\n",
      "train score = 0.8532494758909853\n",
      "dev score = 0.8476190476190476\n",
      "[10,   300] loss: 0.015\n",
      "[10,   600] loss: 0.017\n",
      "train score = 0.8756319514661274\n",
      "dev score = 0.8611111111111112\n",
      "[11,   300] loss: 0.015\n",
      "[11,   600] loss: 0.016\n",
      "train score = 0.8732106339468302\n",
      "dev score = 0.8450704225352113\n",
      "[12,   300] loss: 0.017\n",
      "[12,   600] loss: 0.014\n",
      "train score = 0.88\n",
      "dev score = 0.8506787330316743\n",
      "[13,   300] loss: 0.016\n",
      "[13,   600] loss: 0.014\n",
      "train score = 0.8793619142572284\n",
      "dev score = 0.8584474885844748\n",
      "[14,   300] loss: 0.015\n",
      "[14,   600] loss: 0.014\n",
      "train score = 0.8850806451612904\n",
      "dev score = 0.8558139534883721\n",
      "[15,   300] loss: 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15,   600] loss: 0.013\n",
      "train score = 0.8781985670419653\n",
      "dev score = 0.8598130841121495\n",
      "[16,   300] loss: 0.014\n",
      "[16,   600] loss: 0.015\n",
      "train score = 0.8843813387423936\n",
      "dev score = 0.8598130841121495\n",
      "[17,   300] loss: 0.014\n",
      "[17,   600] loss: 0.015\n",
      "train score = 0.889108910891089\n",
      "dev score = 0.8454545454545456\n",
      "[18,   300] loss: 0.013\n",
      "[18,   600] loss: 0.015\n",
      "train score = 0.8875739644970414\n",
      "dev score = 0.8506787330316743\n",
      "[19,   300] loss: 0.015\n",
      "[19,   600] loss: 0.014\n",
      "train score = 0.8811475409836066\n",
      "dev score = 0.8598130841121495\n",
      "[20,   300] loss: 0.013\n",
      "[20,   600] loss: 0.014\n",
      "train score = 0.888\n",
      "dev score = 0.8623853211009174\n",
      "\n",
      "mean_score = 0.8693969332037392, std = 0.012911986204209908\n"
     ]
    }
   ],
   "source": [
    "seed_num = 5\n",
    "\n",
    "epoch_num = 20\n",
    "hidden_sizes = [200]\n",
    "learning_rate = 0.03\n",
    "batch_size = 6\n",
    "dev_results = []\n",
    "\n",
    "for seed in range(100, 100*seed_num, 100):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    feature_extractor = TfIDFFeatureExtractor(train)\n",
    "    model = BinaryClassificationMLP(feature_extractor.vector_length, hidden_sizes, torch.nn.Tanh)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    max_dev_score = train_model(\n",
    "        epoch_num, train, dev, get_batcher_factory(feature_extractor, batch_size), model, optimizer, criterion)\n",
    "    dev_results.append(max_dev_score)\n",
    "\n",
    "print()\n",
    "print(f'mean_score = {np.mean(dev_results)}, std = {np.std(dev_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.86 F1 is good result! Pipeline works even without complex features and complex classifiers. We can go further with hyperparameter search, deeper networks and final test evaluation but it's just routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
